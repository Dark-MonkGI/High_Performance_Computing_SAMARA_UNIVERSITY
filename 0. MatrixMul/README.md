
## Лабораторная работа по курсу "Высокопроизводительные вычисления".
Samara University <br/>
HPC-2021

## 0.[MatrixMul](https://github.com/Dark-MonkGI/Laboratory-work/blob/2acabac21aadec821bd6a56c421fa41be8692b89/0.%20MatrixMul/HPC_matrix_multi_GPU_ILia_Gr.ipynb)

**Задача:** Реализовать алгоритм перемножения матриц с применением технологии CUDA. <br/>
Реализация должна содержать 2 функции перемножения матриц: на CPU и на GPU
Отчет о проделанной лабораторной работе - это git-репозиторий с исходным кодом
реализации + описание проделанной работы там же в readme.
Необходимо описать реализацию, объяснив, что конкретно было распараллелено и
почему.
Провести эксперименты: перемножить матрицы разных размеров, посчитать
ускорение. Результаты привести в виде таблицы/графика.

**Язык:**  C++ или Python <br/> 
**Входные данные:**  Входные данные: 2 матрицы размером от 100х100 до 2000х2000 каждая. <br/> 
**Выходные данные:**  проверка корректности перемножения + время вычисления. <br/> 

##  **Техническое обеспечение** 
-  Процессор: `Intel(R) Xeon(R) CPU @ 2.30GHz`
-  Графический процессор: `b'Tesla K80'` 
-  Google Colaboratory <br/>
   compute capability: 3.7 
##  **Описание реализации** 

В данной лабораторной работе было произведено перемножение четырех пар матриц, поочередно на CPU и GPU, с применением технологии CUDA.
Выбранный размер матриц для вычислений: <br/>
- 100 х 100
- 300 х 300 
- 700 х 700 
- 1100 х 1100 <br/> 

Данная лабораторная работа выполнялась на языке программирования `"Python 3.7"` , с использованием библиотеки `Numba`.<br/>
> **Overview** <br/>
> Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels
> written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.

В основе данной программы лежат две функции: 
- `CPU_matmul` - функция перемножения матриц на CPU.
- `GPU_matmul` - функция перемножения матриц на GPU, с декораторром `@cuda.jit` <br/>

Numba @jit декоратор в основном работает в двух режимах компиляции, nopython-режим и object-режим. Поведение nopythonрежим компиляции состоит в том, чтобы скомпилировать декорированную функцию так, чтобы она работала полностью без участия интерпретатора Python. Это рекомендуемый и лучший способ использования Numba @jit-декоратор, поскольку он ведет к лучшему представлению.
![logo](https://images1.russianblogs.com/136/1d/1d8a40f737865045aa5852c0a0aea0e0.JPEG)
##  **Результаты вычислений** 
В таблице приведены результаты расчета случайно сгенерированных матриц на CPU и GPU соответственно: 
 	 № |Device|Time(hh:mm:ss)| Matrix_size | 
:-----:|:-----:|:-----:|:-----:|
0 | CPU | 0:00:01 | 100 x 100 |
1 | GPU | 0:00:00.23 | 100 x 100 |
2 | CPU | 0:00:17 | 300 x 300 | 
3 | GPU | 0:00:00.02 | 300 x 300 |
4 | CPU | 0:03:42 | 700 x 700 |
5 | GPU | 0:00:00.12 | 700 x 700 |
6 | CPU | 0:13:57 | 1100 x 1100 |
7 | GPU | 0:00:00.39 | 1100 x 1100 |
<br/> 

 ##  **Вывод** 
  По таблице наглядно видно, что выполнение вычислений на GPU, дает прирост производительности, при распараллеливании на потоки в CUDA - ядрах. Малый размер матриц, не так наглядно показывает рост производительности, но на матрицах размера побольше разница - колоссальная. <br/> 
  Если задача в проекте подразумевает интенсивные вычисления и есть возможность загрузить большую часть данных на карту до вычислений (закешировать), то стоит задуматься об использовании GPU в своих проектах. Учитывая заранее, сколько будет параллельных запросов и достаточно ли одной карты для нагрузки или нужен сервер, с несколькими картами или кластер GPU-серверов.
