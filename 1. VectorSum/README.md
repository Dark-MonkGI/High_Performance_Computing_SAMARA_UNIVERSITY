## Лабораторная работа по курсу "Высокопроизводительные вычисления".<br/>
### *Сложение векторов с применением технологии CUDA.* <br/>
Samara University <br/>
HPC-2021

## 1.[VectorSum](https://github.com/Dark-MonkGI/Laboratory-work/blob/8da7243a5a189b13cc9937f302980dcf433fccf5/1.%20VectorSum/HPC_Vector_GPU_ILia.ipynb)

**Задача:** Реализовать алгоритм сложения элементов вектора с применением технологии CUDA. <br/>
- Входные данные: Вектор размером от: 1 000..1 000 000 значений. <br/>
- Выходные данные: сумма элементов вектора + время вычисления <br/>
Реализация должна содержать 2 функции сложения элементов вектора: на CPU и на GPU с применением CUDA. <br/>
Отчет о проделанной лабораторной работе - это git-репозиторий с исходным кодом
реализации + описание проделанной работы там же в readme.
Необходимо описать реализацию, объяснив, что конкретно было распараллелено и
почему.
Провести эксперименты: получить сумму векторов разных размеров (провести 5 или
более экспериментов), посчитать ускорение. Результаты привести в виде
таблицы/графика. <br/>
**Язык:**  C++ или Python <br/> 
**Входные данные:** 5 векторов, размером: <br/>
- 10 000
- 25 000 000
- 50 000 000
- 75 000 000
- 100 000 000 <br/> 
**Выходные данные:**  Два списка результатов сложения векторов на CPU и GPU соответсвенно. <br/> 
Проверка корректности сложения + таблица времени вычисления. <br/> 

##  **Техническое обеспечение** 
-  Процессор: `Intel(R) Xeon(R) CPU @ 2.30GHz`
-  Графический процессор: `b'Tesla K80'` 
-  Google Colaboratory <br/>
   compute capability: 3.7 
##  **Описание реализации** 

Данная работа разбита на пять основынх разделов, где:
1. Раздел - Импорт библиотек, подключение CUDA devices.
2. Раздел - Обьявляем переменные, задаем размер векторов.
3. Раздел - Сложение векторов на CPU (реализованна функция сложения векторов).
4. Раздел - Сложение векторов на GPU, в данном разделе реализованны две основные функции:
   - Функция сложения векторов, с применением numba @jit декоратором, непосредсвенно на CUDA devices.
   - Функция копирования данных в пямять devices, предварительно зается размер блоков и колличество нитей в блоке.
5. Раздел итогов. Проводится проверка верности сложения векторов на CPU и GPU. Выводиться таблица времени выполения. <br/> 

Данная лабораторная работа выполнялась на языке программирования `"Python 3.7"` , с использованием библиотеки `Numba`.<br/>

> **Overview** <br/>
> Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels
> written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.


Numba @jit декоратор в основном работает в двух режимах компиляции, nopython-режим и object-режим. Поведение nopythonрежим компиляции состоит в том, чтобы скомпилировать декорированную функцию так, чтобы она работала полностью без участия интерпретатора Python. Это рекомендуемый и лучший способ использования Numba @jit-декоратор, поскольку он ведет к лучшему представлению.
![logo](https://images.slideplayer.com/15/4844147/slides/slide_4.jpg)
![logo](https://i2.wp.com/thg.ru/graphic/nvidia_cuda/images/005_resize.png)
##  **Результаты вычислений** 
В таблице приведены результаты расчета случайно сгенерированных матриц на CPU и GPU соответственно: 
 	 № |Device|Time(hh:mm:ss)| Matrix_size | 
:-----:|:-----:|:-----:|:-----:|
0 | CPU | 0:00:01 | 100 x 100 |
1 | GPU | 0:00:00.23 | 100 x 100 |
2 | CPU | 0:00:17 | 300 x 300 | 
3 | GPU | 0:00:00.02 | 300 x 300 |
4 | CPU | 0:03:42 | 700 x 700 |
5 | GPU | 0:00:00.12 | 700 x 700 |
6 | CPU | 0:13:57 | 1100 x 1100 |
7 | GPU | 0:00:00.39 | 1100 x 1100 |
<br/> 

 ##  **Вывод** 
  По таблице наглядно видно, что выполнение вычислений на GPU, дает прирост производительности, при распараллеливании на потоки в CUDA - ядрах. Малый размер матриц, не так наглядно показывает рост производительности, но на матрицах размера побольше разница - колоссальная. <br/> 
  Если задача в проекте подразумевает интенсивные вычисления и есть возможность загрузить большую часть данных на карту до вычислений (закешировать), то стоит задуматься об использовании GPU в своих проектах. Учитывая заранее, сколько будет параллельных запросов и достаточно ли одной карты для нагрузки или нужен сервер, с несколькими картами или кластер GPU-серверов.
